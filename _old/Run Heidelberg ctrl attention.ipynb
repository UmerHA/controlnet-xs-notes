{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda36831-a1e7-4c2f-b3ab-5f45dfaac14e",
   "metadata": {},
   "source": [
    "In this notebook, I run one step for TwoStreamControlModel to get intermediate outputs which I can then compare to my local run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b74b9e-7bbc-41ff-bc01-3d3055570a57",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43bd7c1-8c6b-4f3c-bf65-5398fdbbdc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # needed to make torch deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1c7d13-020e-4053-9754-7f2bf5b4829f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import scripts.control_utils as cu\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad3a198-093b-46fa-9cda-a13750fc9aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be14a47f-ac71-4e60-a52f-4befa3fb0b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_config = 'cnxs_config/sdxl/sdxl_encD_canny_48m.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6552dd-0ad5-4db2-b1a6-3c9b8ca55f4f",
   "metadata": {},
   "source": [
    "If this results in the kernel crashing, I'm using too much GPU memory elsewhere. Shut down every other kernel and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70915bf0-38be-40e8-a791-23a6ded976ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 320, out-chn: 320, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 640, out-chn: 640, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 352, out-chn: 32, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 2 w/ 64 channels and 1 heads\n",
      "constructing SpatialTransformer of depth 2 w/ 64 channels and 1 heads\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 704, out-chn: 64, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "Initialized embedder #0: FrozenCLIPEmbedder with 123060480 params. Trainable: False\n",
      "Initialized embedder #1: FrozenOpenCLIPEmbedder2 with 694659841 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Loaded model config from [cnxs_config/sdxl/sdxl_encD_canny_48m.yaml]\n"
     ]
    }
   ],
   "source": [
    "model = cu.create_model(path_to_config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5718b-1db0-4620-b084-ec490dc71286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ffaa6e-f630-423d-ac72-cb632cd4e306",
   "metadata": {},
   "source": [
    "Load output before 1st control attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a3ee3a6-9a2a-477e-a4c4-d8bf20b3808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def tensor_file(i): return f'logs/cloud/tensor_{i}.pt'\n",
    "\n",
    "inp = SimpleNamespace(\n",
    "    context = SimpleNamespace(t=torch.load(tensor_file(26), map_location='cuda'), expected_err=0.000),\n",
    "    proj_in = SimpleNamespace(t=torch.load(tensor_file(44), map_location='cuda'), expected_err=0.000),\n",
    "    attn1   = SimpleNamespace(t=torch.load(tensor_file(45), map_location='cuda'), expected_err=0.054),\n",
    "    attn2   = SimpleNamespace(t=torch.load(tensor_file(49), map_location='cuda'), expected_err=0.627),\n",
    "    feedf   = SimpleNamespace(t=torch.load(tensor_file(51), map_location='cuda'), expected_err=0.037),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0774e-9737-48b8-af9b-f3dd3e13a33a",
   "metadata": {},
   "source": [
    "Reminder: This is what a transformer block does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c2653-2c62-4ccb-aa92-89f353b85aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h += attn1(norm1(h)) # 1. Self-Attention\n",
    "# h += attn2(norm2(h)) # 2. Cross-Attention\n",
    "# h += ff(norm3(h))    # 3. Feed-forward\n",
    "# return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5371940a-5365-4b3e-85b1-d50f43976833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialTransformer(\n",
       "  (norm): GroupNorm_leq32(32, 64, eps=1e-05, affine=True)\n",
       "  (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-1): 2 x BasicTransformerBlock(\n",
       "      (attn1): MemoryEfficientCrossAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_out): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=64, out_features=512, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn2): MemoryEfficientCrossAttention(\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (to_k): Linear(in_features=2048, out_features=64, bias=False)\n",
       "        (to_v): Linear(in_features=2048, out_features=64, bias=False)\n",
       "        (to_out): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_block = model.model.control_model.input_blocks[4][1]\n",
    "attn_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4b614cb-49d8-4e28-8cfe-d60e75c154e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm1 = attn_block.transformer_blocks[0].norm1\n",
    "attn1 = attn_block.transformer_blocks[0].attn1\n",
    "norm2 = attn_block.transformer_blocks[0].norm2\n",
    "attn2 = attn_block.transformer_blocks[0].attn2\n",
    "norm3 = attn_block.transformer_blocks[0].norm3\n",
    "feedf = attn_block.transformer_blocks[0].ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57c294c2-24c9-4dea-9775-ed7f7ac5238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = inp.proj_in.t\n",
    "\n",
    "x_norm1 = norm1(h)\n",
    "x_attn1 = attn1(x_norm1)\n",
    "h += x_attn1\n",
    "\n",
    "x_norm2 = norm2(h)\n",
    "x_attn2 = attn2(x_norm2, inp.context.t)\n",
    "h += x_attn2\n",
    "\n",
    "x_norm3 = norm3(h)\n",
    "x_feedf = feedf(x_norm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f86e77cf-2db3-481a-ad79-e9bad1321fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae(t1,t2):\n",
    "    if t1.shape!=t1.shape: raise ValueError(\"Shapes don't match\")\n",
    "    return (t1-t2).abs().mean().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "602acaba-3cdf-4818-9467-5fa7045d30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x_attn1, inp.attn1.t), torch.allclose(x_attn2, inp.attn2.t), torch.allclose(x_feedf, inp.feedf.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a590c-e15f-478b-a9ce-fd8afb481883",
   "metadata": {},
   "source": [
    "Let's compute attention wrong, by ingnoring to add any attn1/attn2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c646fa6d-90d4-4a20-ab55-a7d4b2ef605a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_x_norm1 = norm1(inp.proj_in.t)\n",
    "bad_x_attn1 = attn1(bad_x_norm1)\n",
    "\n",
    "bad_x_norm2 = norm2(bad_x_attn1)\n",
    "bad_x_attn2 = attn2(bad_x_norm2, inp.context.t)\n",
    "\n",
    "bad_x_norm3 = norm3(bad_x_attn2)\n",
    "bad_x_feedf = feedf(bad_x_norm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "117027a1-f386-444a-b66c-9f97f558f11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(0.7726, device='cuda:0'),\n",
       " tensor(0.0606, device='cuda:0'))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae(bad_x_attn1, inp.attn1.t), mae(bad_x_attn2, inp.attn2.t), mae(bad_x_feedf, inp.feedf.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0d18c-4d7c-4b83-b311-84ad962aa0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5f6d6dd-c40d-4047-8bc2-7577e11a79e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=3)\n",
    "def print_head(m): print(m.weight.data.flatten()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a543a9ce-ae08-45c2-84b4-691db8f56887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.071, -0.062, -0.003, -0.056,  0.030, -0.003, -0.015, -0.142, -0.017, -0.075], device='cuda:0')\n",
      "tensor([ 0.073,  0.008,  0.053, -0.025,  0.056,  0.094,  0.037, -0.085,  0.065, -0.010], device='cuda:0')\n",
      "tensor([-0.092,  0.013,  0.104, -0.014, -0.075, -0.126, -0.028, -0.007,  0.080,  0.094], device='cuda:0')\n",
      "tensor([-0.071, -0.053, -0.049, -0.112, -0.016, -0.115, -0.001,  0.109,  0.055,  0.020], device='cuda:0')\n",
      "tensor([ 0.091,  0.001, -0.058,  0.122, -0.091, -0.073,  0.009,  0.032,  0.011,  0.037], device='cuda:0')\n",
      "tensor([ 0.003, -0.020,  0.015,  0.005, -0.025, -0.025, -0.016,  0.003,  0.009,  0.024], device='cuda:0')\n",
      "tensor([ 0.021, -0.021, -0.020, -0.028, -0.007,  0.028,  0.028, -0.014,  0.003,  0.029], device='cuda:0')\n",
      "tensor([-0.042, -0.100, -0.041,  0.014,  0.087,  0.011, -0.054,  0.069,  0.096, -0.027], device='cuda:0')\n",
      "tensor([-0.015,  0.029, -0.066, -0.034, -0.022, -0.020, -0.085, -0.030, -0.075, -0.012], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print_head(attn1.to_q)\n",
    "print_head(attn1.to_k)\n",
    "print_head(attn1.to_v)\n",
    "print_head(attn1.to_out[0])\n",
    "\n",
    "print_head(attn2.to_q)\n",
    "print_head(attn2.to_k)\n",
    "print_head(attn2.to_v)\n",
    "print_head(attn2.to_out[0])\n",
    "\n",
    "print_head(feedf.net[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe6c7e-616d-4b2e-b250-16f64a4b6238",
   "metadata": {},
   "source": [
    "These looks identical to the diffusers weights.\n",
    "\n",
    "**So, we know:**\n",
    "- cloud modules(cloud inp) = cloud outp\n",
    "- local modules(cloud inp) = ~loca outp\n",
    "- error starts after 1st cross attn, so the self-attn seems to be okay\n",
    "\n",
    "Therefore, cloud modules and local modules must be somehow different.\n",
    "But their weights look identical.\n",
    "\n",
    "**Hypothesis:** cloud uses MemoryEfficientCrossAttention, maybe that procudes a difference?\n",
    "\n",
    "**How to test:** Zoom further into 1st cross attn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85623f5a-5837-4585-bb14-ba9aa9a2adf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a8748-8e3e-4059-8862-41f186ac2132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
