{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721ce825-126e-4e68-a41e-6a8a3a381307",
   "metadata": {},
   "source": [
    "This is the part from controlnet-xs where the controlnet and basenet are executed together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef34458-4184-4ebf-ab86-e6cc445169b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a56f63-c9ad-4cfd-bfa3-c853e1d60c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2337dbf6-1ca0-41fc-bc99-79bd3f4265b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"Any module where forward() takes timestep embeddings as a second argument.\"\"\"\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb): pass # Apply the module to `x` given `emb` timestep embeddings.\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"A sequential module that passes timestep embeddings to the children that support it as an extra input.\"\"\"\n",
    "    def forward(self, x, emb, context=None, skip_time_mix=False, time_context=None, num_video_frames=None, time_context_cat=None, use_crossframe_attention_in_spatial_layers=False):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock): x = layer(x, emb)\n",
    "            elif isinstance(layer, SpatialTransformer): x = layer(x, context)\n",
    "            elif layer.__class__.__name__ == 'SpatialTransformer': x = layer(x, context)\n",
    "            else: x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60476995-5a38-4fc1-921d-4c4495c0dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStreamControlNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels, model_channels, out_channels, hint_channels,\n",
    "            num_res_blocks,\n",
    "            attention_resolutions,\n",
    "            dropout=0,\n",
    "            channel_mult=(1, 2, 4, 8),\n",
    "            conv_resample=True,\n",
    "            dims=2,\n",
    "            use_checkpoint=False,\n",
    "            use_fp16=False,\n",
    "            num_heads=-1, num_head_channels=-1, num_heads_upsample=-1,\n",
    "            use_scale_shift_norm=False,\n",
    "            resblock_updown=False,\n",
    "            use_new_attention_order=False,\n",
    "            adm_in_channels=None,\n",
    "            use_spatial_transformer=False,  # custom transformer support\n",
    "            transformer_depth=1,  # custom transformer support\n",
    "            context_dim=None,  # custom transformer support\n",
    "            n_embed=None,  # custom support for prediction of discrete ids into codebook of first stage vq model\n",
    "            legacy=False,\n",
    "            spatial_transformer_attn_type=\"softmax\",\n",
    "            use_linear_in_transformer=False,\n",
    "            num_classes=None,\n",
    "            control_model_ratio=1.0,    # ratio of the control model size compared to the base model. [0, 1]\n",
    "            base_model=None,\n",
    "            learn_embedding=False,\n",
    "            control_mode='canny',\n",
    "        ):\n",
    "        # Umer: Let's fix some params to make the code easier\n",
    "        infusion2control = 'cat' # how to infuse intermediate information into the control net? {'add', 'cat', None}\n",
    "        infusion2base = 'add'    # how to infuse intermediate information into the base net? {'add', 'cat'}\n",
    "        guiding='encoder',       # use just encoder for control or the whole encoder + decoder net? {'encoder', 'encoder_double', 'full'}\n",
    "        two_stream_mode='cross', # mode for the two stream infusion. {'cross', 'sequential'}\n",
    "        # # #\n",
    "            \n",
    "        super().__init__()\n",
    "        \n",
    "        self.control_mode = control_mode\n",
    "        self.learn_embedding = learn_embedding\n",
    "        self.infusion2control = infusion2control\n",
    "        self.infusion2base = infusion2base\n",
    "        self.in_ch_factor = 1 if infusion2control == 'add' else 2\n",
    "        self.guiding = guiding\n",
    "        self.two_stream_mode = two_stream_mode\n",
    "        self.control_model_ratio = control_model_ratio\n",
    "        self.out_channels = out_channels\n",
    "        self.dims = 2\n",
    "        self.model_channels = model_channels\n",
    "        self.no_control = False\n",
    "        self.control_scale = 1.0\n",
    "    \n",
    "        self.hint_model = None\n",
    "        \n",
    "        ################# start control model variations #################\n",
    "        if base_model is None:\n",
    "            base_model = UNetModel(\n",
    "                adm_in_channels=adm_in_channels, num_classes=num_classes, use_checkpoint=use_checkpoint,\n",
    "                in_channels=in_channels, out_channels=out_channels, model_channels=model_channels,\n",
    "                attention_resolutions=attention_resolutions, num_res_blocks=num_res_blocks,\n",
    "                channel_mult=channel_mult, num_head_channels=num_head_channels, use_spatial_transformer=use_spatial_transformer,\n",
    "                use_linear_in_transformer=use_linear_in_transformer, transformer_depth=transformer_depth,\n",
    "                context_dim=context_dim, spatial_transformer_attn_type=spatial_transformer_attn_type,\n",
    "                legacy=legacy, dropout=dropout,\n",
    "                conv_resample=conv_resample, dims=dims, use_fp16=use_fp16, num_heads=num_heads,\n",
    "                num_heads_upsample=num_heads_upsample, use_scale_shift_norm=use_scale_shift_norm,\n",
    "                resblock_updown=resblock_updown, use_new_attention_order=use_new_attention_order,\n",
    "                n_embed=n_embed,\n",
    "            )\n",
    "    \n",
    "        self.control_model = ControlledXLUNetModel(\n",
    "            adm_in_channels=adm_in_channels, num_classes=num_classes, use_checkpoint=use_checkpoint,\n",
    "            in_channels=in_channels, out_channels=out_channels, model_channels=model_channels,\n",
    "            attention_resolutions=attention_resolutions, num_res_blocks=num_res_blocks,\n",
    "            channel_mult=channel_mult, num_head_channels=num_head_channels, use_spatial_transformer=use_spatial_transformer,\n",
    "            use_linear_in_transformer=use_linear_in_transformer, transformer_depth=transformer_depth,\n",
    "            context_dim=context_dim, spatial_transformer_attn_type=spatial_transformer_attn_type,\n",
    "            legacy=legacy, dropout=dropout,\n",
    "            conv_resample=conv_resample, dims=dims, use_fp16=use_fp16, num_heads=num_heads,\n",
    "            num_heads_upsample=num_heads_upsample, use_scale_shift_norm=use_scale_shift_norm,\n",
    "            resblock_updown=resblock_updown, use_new_attention_order=use_new_attention_order,\n",
    "            n_embed=n_embed,\n",
    "            infusion2control=infusion2control,\n",
    "            guiding=guiding, two_stream_mode=two_stream_mode, control_model_ratio=control_model_ratio,\n",
    "        )\n",
    "    \n",
    "        self.diffusion_model = base_model\n",
    "        ################# end control model variations #################\n",
    "    \n",
    "        self.enc_zero_convs_out = nn.ModuleList([])\n",
    "        self.enc_zero_convs_in = nn.ModuleList([])\n",
    "    \n",
    "        self.middle_block_out = nn.ModuleList([])\n",
    "        self.middle_block_in = nn.ModuleList([])\n",
    "    \n",
    "        self.dec_zero_convs_out = nn.ModuleList([])\n",
    "        self.dec_zero_convs_in = nn.ModuleList([])\n",
    "    \n",
    "        ch_inout_ctr = {'enc': [], 'mid': [], 'dec': []}\n",
    "        ch_inout_base = {'enc': [], 'mid': [], 'dec': []}\n",
    "    \n",
    "        ################# Gather Channel Sizes #################\n",
    "        for module in self.control_model.input_blocks:\n",
    "            if isinstance(module[0], nn.Conv2d):\n",
    "                ch_inout_ctr['enc'].append((module[0].in_channels, module[0].out_channels))\n",
    "            elif isinstance(module[0], (ResBlock, ResBlock_orig)):\n",
    "                ch_inout_ctr['enc'].append((module[0].channels, module[0].out_channels))\n",
    "            elif isinstance(module[0], Downsample):\n",
    "                ch_inout_ctr['enc'].append((module[0].channels, module[-1].out_channels))\n",
    "    \n",
    "        for module in base_model.input_blocks:\n",
    "            if isinstance(module[0], nn.Conv2d):\n",
    "                ch_inout_base['enc'].append((module[0].in_channels, module[0].out_channels))\n",
    "            elif isinstance(module[0], (ResBlock, ResBlock_orig)):\n",
    "                ch_inout_base['enc'].append((module[0].channels, module[0].out_channels))\n",
    "            elif isinstance(module[0], Downsample):\n",
    "                ch_inout_base['enc'].append((module[0].channels, module[-1].out_channels))\n",
    "    \n",
    "        ch_inout_ctr['mid'].append((self.control_model.middle_block[0].channels, self.control_model.middle_block[-1].out_channels))\n",
    "        ch_inout_base['mid'].append((base_model.middle_block[0].channels, base_model.middle_block[-1].out_channels))\n",
    "    \n",
    "        # guiding == 'encoder'\n",
    "    \n",
    "        for module in base_model.output_blocks:\n",
    "            if isinstance(module[0], nn.Conv2d):\n",
    "                ch_inout_base['dec'].append((module[0].in_channels, module[0].out_channels))\n",
    "            elif isinstance(module[0], (ResBlock, ResBlock_orig)):\n",
    "                ch_inout_base['dec'].append((module[0].channels, module[0].out_channels))\n",
    "            elif isinstance(module[-1], Upsample):\n",
    "                ch_inout_base['dec'].append((module[0].channels, module[-1].out_channels))\n",
    "    \n",
    "        self.ch_inout_ctr = ch_inout_ctr\n",
    "        self.ch_inout_base = ch_inout_base\n",
    "    \n",
    "        ################# Build zero convolutions #################\n",
    "        # two_stream_mode == 'cross'\n",
    "        ################# cross infusion #################\n",
    "        # infusion2control == 'cat'  (ie processing full concatenation (all output layers are concatenated without \"slimming\"))\n",
    "        for ch_io_base in ch_inout_base['enc']:\n",
    "            self.enc_zero_convs_in.append(self.make_zero_conv(\n",
    "                in_channels=ch_io_base[1], out_channels=ch_io_base[1])\n",
    "            )\n",
    "            # guiding == 'encoder'\n",
    "    \n",
    "    \n",
    "        # infusion2base (- consider all three guidings) == 'add'\n",
    "        self.middle_block_out = self.make_zero_conv(ch_inout_ctr['mid'][-1][1], ch_inout_base['mid'][-1][1])\n",
    "        \n",
    "        # guiding == 'encoder'\n",
    "        self.dec_zero_convs_out.append(\n",
    "            self.make_zero_conv(ch_inout_ctr['enc'][-1][1], ch_inout_base['mid'][-1][1])\n",
    "        )\n",
    "        for i in range(1, len(ch_inout_ctr['enc'])):\n",
    "            self.dec_zero_convs_out.append(\n",
    "                self.make_zero_conv(ch_inout_ctr['enc'][-(i + 1)][1], ch_inout_base['dec'][i - 1][1])\n",
    "            )\n",
    "    \n",
    "        \n",
    "        self.input_hint_block = TimestepEmbedSequential(\n",
    "            conv_nd(dims, hint_channels, 16, 3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, 16, 16, 3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, 16, 32, 3, padding=1, stride=2),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, 32, 32, 3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, 32, 96, 3, padding=1, stride=2),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, 96, 96, 3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, 96, 256, 3, padding=1, stride=2),\n",
    "            nn.SiLU(),\n",
    "            zero_module(conv_nd(dims, 256, int(model_channels * control_model_ratio), 3, padding=1))\n",
    "        )\n",
    "    \n",
    "        scale_list = [1.] * len(self.enc_zero_convs_out) + [1.] + [1.] * len(self.dec_zero_convs_out)\n",
    "        self.register_buffer('scale_list', torch.tensor(scale_list))\n",
    "\n",
    "    def make_zero_conv(self, in_channels, out_channels=None):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels or in_channels\n",
    "        return TimestepEmbedSequential(\n",
    "            zero_module(conv_nd(self.dims, in_channels, out_channels, 1, padding=0))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b52d6-f9ba-4bbc-838d-c26f79160e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d62ee4d-062c-447e-8b3d-d8cf091d22a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TwoStreamControlNet.__init__() missing 6 required positional arguments: 'in_channels', 'model_channels', 'out_channels', 'hint_channels', 'num_res_blocks', and 'attention_resolutions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mTwoStreamControlNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TwoStreamControlNet.__init__() missing 6 required positional arguments: 'in_channels', 'model_channels', 'out_channels', 'hint_channels', 'num_res_blocks', and 'attention_resolutions'"
     ]
    }
   ],
   "source": [
    "TwoStreamControlNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a1db6-7539-49a9-bb02-39a99110ff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
