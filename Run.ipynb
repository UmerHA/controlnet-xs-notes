{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ed5de-0c21-4916-b808-5d344e208c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ControlNet-XS/requirements/pt2.txt -qq\n",
    "!pip install -e ControlNet-XS -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2a7fec-d703-4f01-ab2b-b551f0d73f72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease              \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease                   \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libgl1 libgl1-mesa-dri libglapi-mesa libglvnd0 libglx-mesa0\n",
      "  libglx0 libllvm10 libpciaccess0 libsensors4 libx11-xcb1 libxcb-dri2-0\n",
      "  libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-sync1 libxdamage1\n",
      "  libxfixes3 libxshmfence1 libxxf86vm1\n",
      "Suggested packages:\n",
      "  pciutils lm-sensors\n",
      "The following NEW packages will be installed:\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libgl1 libgl1-mesa-dri libgl1-mesa-glx libglapi-mesa\n",
      "  libglvnd0 libglx-mesa0 libglx0 libllvm10 libpciaccess0 libsensors4\n",
      "  libx11-xcb1 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0\n",
      "  libxcb-sync1 libxdamage1 libxfixes3 libxshmfence1 libxxf86vm1\n",
      "0 upgraded, 27 newly installed, 0 to remove and 60 not upgraded.\n",
      "Need to get 25.4 MB of archives.\n",
      "After this operation, 327 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86vm1 amd64 1:1.1.4-1 [10.6 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libelf1 amd64 0.170-0.4ubuntu0.1 [44.8 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdrm-common all 2.4.101-2~18.04.1 [5560 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdrm2 amd64 2.4.101-2~18.04.1 [32.3 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdrm-amdgpu1 amd64 2.4.101-2~18.04.1 [18.2 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpciaccess0 amd64 0.14-1 [17.9 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdrm-intel1 amd64 2.4.101-2~18.04.1 [60.0 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdrm-nouveau2 amd64 2.4.101-2~18.04.1 [16.5 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdrm-radeon1 amd64 2.4.101-2~18.04.1 [21.7 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglapi-mesa amd64 20.0.8-0ubuntu1~18.04.1 [26.6 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libllvm10 amd64 1:10.0.0-4ubuntu1~18.04.2 [15.4 MB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsensors4 amd64 1:3.4.0-4ubuntu0.1 [28.3 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-dri amd64 20.0.8-0ubuntu1~18.04.1 [9333 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglvnd0 amd64 1.0.0-2ubuntu2.3 [47.0 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-xcb1 amd64 2:1.6.4-3ubuntu0.4 [9720 B]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-dri2-0 amd64 1.13-2~ubuntu18.04 [6920 B]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-dri3-0 amd64 1.13-2~ubuntu18.04 [6568 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-glx0 amd64 1.13-2~ubuntu18.04 [22.1 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-present0 amd64 1.13-2~ubuntu18.04 [5552 B]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-sync1 amd64 1.13-2~ubuntu18.04 [8808 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxdamage1 amd64 1:1.1.4-3 [6934 B]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxfixes3 amd64 1:5.0.3-1 [10.8 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxshmfence1 amd64 1.3-1 [5028 B]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglx-mesa0 amd64 20.0.8-0ubuntu1~18.04.1 [139 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglx0 amd64 1.0.0-2ubuntu2.3 [28.1 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1 amd64 1.0.0-2ubuntu2.3 [86.2 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 20.0.8-0ubuntu1~18.04.1 [5532 B]\n",
      "Fetched 25.4 MB in 5s (5419 kB/s)       \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libxxf86vm1:amd64.\n",
      "(Reading database ... 21062 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libxxf86vm1_1%3a1.1.4-1_amd64.deb ...\n",
      "Unpacking libxxf86vm1:amd64 (1:1.1.4-1) ...\n",
      "Selecting previously unselected package libelf1:amd64.\n",
      "Preparing to unpack .../01-libelf1_0.170-0.4ubuntu0.1_amd64.deb ...\n",
      "Unpacking libelf1:amd64 (0.170-0.4ubuntu0.1) ...\n",
      "Selecting previously unselected package libdrm-common.\n",
      "Preparing to unpack .../02-libdrm-common_2.4.101-2~18.04.1_all.deb ...\n",
      "Unpacking libdrm-common (2.4.101-2~18.04.1) ...\n",
      "Selecting previously unselected package libdrm2:amd64.\n",
      "Preparing to unpack .../03-libdrm2_2.4.101-2~18.04.1_amd64.deb ...\n",
      "Unpacking libdrm2:amd64 (2.4.101-2~18.04.1) ...\n",
      "Selecting previously unselected package libdrm-amdgpu1:amd64.\n",
      "Preparing to unpack .../04-libdrm-amdgpu1_2.4.101-2~18.04.1_amd64.deb ...\n",
      "Unpacking libdrm-amdgpu1:amd64 (2.4.101-2~18.04.1) ...\n",
      "Selecting previously unselected package libpciaccess0:amd64.\n",
      "Preparing to unpack .../05-libpciaccess0_0.14-1_amd64.deb ...\n",
      "Unpacking libpciaccess0:amd64 (0.14-1) ...\n",
      "Selecting previously unselected package libdrm-intel1:amd64.\n",
      "Preparing to unpack .../06-libdrm-intel1_2.4.101-2~18.04.1_amd64.deb ...\n",
      "Unpacking libdrm-intel1:amd64 (2.4.101-2~18.04.1) ...\n",
      "Selecting previously unselected package libdrm-nouveau2:amd64.\n",
      "Preparing to unpack .../07-libdrm-nouveau2_2.4.101-2~18.04.1_amd64.deb ...\n",
      "Unpacking libdrm-nouveau2:amd64 (2.4.101-2~18.04.1) ...\n",
      "Selecting previously unselected package libdrm-radeon1:amd64.\n",
      "Preparing to unpack .../08-libdrm-radeon1_2.4.101-2~18.04.1_amd64.deb ...\n",
      "Unpacking libdrm-radeon1:amd64 (2.4.101-2~18.04.1) ...\n",
      "Selecting previously unselected package libglapi-mesa:amd64.\n",
      "Preparing to unpack .../09-libglapi-mesa_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libglapi-mesa:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package libllvm10:amd64.\n",
      "Preparing to unpack .../10-libllvm10_1%3a10.0.0-4ubuntu1~18.04.2_amd64.deb ...\n",
      "Unpacking libllvm10:amd64 (1:10.0.0-4ubuntu1~18.04.2) ...\n",
      "Selecting previously unselected package libsensors4:amd64.\n",
      "Preparing to unpack .../11-libsensors4_1%3a3.4.0-4ubuntu0.1_amd64.deb ...\n",
      "Unpacking libsensors4:amd64 (1:3.4.0-4ubuntu0.1) ...\n",
      "Selecting previously unselected package libgl1-mesa-dri:amd64.\n",
      "Preparing to unpack .../12-libgl1-mesa-dri_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dri:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package libglvnd0:amd64.\n",
      "Preparing to unpack .../13-libglvnd0_1.0.0-2ubuntu2.3_amd64.deb ...\n",
      "Unpacking libglvnd0:amd64 (1.0.0-2ubuntu2.3) ...\n",
      "Selecting previously unselected package libx11-xcb1:amd64.\n",
      "Preparing to unpack .../14-libx11-xcb1_2%3a1.6.4-3ubuntu0.4_amd64.deb ...\n",
      "Unpacking libx11-xcb1:amd64 (2:1.6.4-3ubuntu0.4) ...\n",
      "Selecting previously unselected package libxcb-dri2-0:amd64.\n",
      "Preparing to unpack .../15-libxcb-dri2-0_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "Unpacking libxcb-dri2-0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Selecting previously unselected package libxcb-dri3-0:amd64.\n",
      "Preparing to unpack .../16-libxcb-dri3-0_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "Unpacking libxcb-dri3-0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Selecting previously unselected package libxcb-glx0:amd64.\n",
      "Preparing to unpack .../17-libxcb-glx0_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "Unpacking libxcb-glx0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Selecting previously unselected package libxcb-present0:amd64.\n",
      "Preparing to unpack .../18-libxcb-present0_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "Unpacking libxcb-present0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Selecting previously unselected package libxcb-sync1:amd64.\n",
      "Preparing to unpack .../19-libxcb-sync1_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "Unpacking libxcb-sync1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Selecting previously unselected package libxdamage1:amd64.\n",
      "Preparing to unpack .../20-libxdamage1_1%3a1.1.4-3_amd64.deb ...\n",
      "Unpacking libxdamage1:amd64 (1:1.1.4-3) ...\n",
      "Selecting previously unselected package libxfixes3:amd64.\n",
      "Preparing to unpack .../21-libxfixes3_1%3a5.0.3-1_amd64.deb ...\n",
      "Unpacking libxfixes3:amd64 (1:5.0.3-1) ...\n",
      "Selecting previously unselected package libxshmfence1:amd64.\n",
      "Preparing to unpack .../22-libxshmfence1_1.3-1_amd64.deb ...\n",
      "Unpacking libxshmfence1:amd64 (1.3-1) ...\n",
      "Selecting previously unselected package libglx-mesa0:amd64.\n",
      "Preparing to unpack .../23-libglx-mesa0_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libglx-mesa0:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package libglx0:amd64.\n",
      "Preparing to unpack .../24-libglx0_1.0.0-2ubuntu2.3_amd64.deb ...\n",
      "Unpacking libglx0:amd64 (1.0.0-2ubuntu2.3) ...\n",
      "Selecting previously unselected package libgl1:amd64.\n",
      "Preparing to unpack .../25-libgl1_1.0.0-2ubuntu2.3_amd64.deb ...\n",
      "Unpacking libgl1:amd64 (1.0.0-2ubuntu2.3) ...\n",
      "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
      "Preparing to unpack .../26-libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libxcb-present0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Setting up libglvnd0:amd64 (1.0.0-2ubuntu2.3) ...\n",
      "Setting up libxcb-dri2-0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Setting up libxcb-dri3-0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Setting up libxcb-glx0:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Setting up libllvm10:amd64 (1:10.0.0-4ubuntu1~18.04.2) ...\n",
      "Setting up libxdamage1:amd64 (1:1.1.4-3) ...\n",
      "Setting up libxfixes3:amd64 (1:5.0.3-1) ...\n",
      "Setting up libelf1:amd64 (0.170-0.4ubuntu0.1) ...\n",
      "Setting up libxshmfence1:amd64 (1.3-1) ...\n",
      "Setting up libglapi-mesa:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libdrm-common (2.4.101-2~18.04.1) ...\n",
      "Setting up libxcb-sync1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Setting up libx11-xcb1:amd64 (2:1.6.4-3ubuntu0.4) ...\n",
      "Setting up libpciaccess0:amd64 (0.14-1) ...\n",
      "Setting up libsensors4:amd64 (1:3.4.0-4ubuntu0.1) ...\n",
      "Setting up libxxf86vm1:amd64 (1:1.1.4-1) ...\n",
      "Setting up libdrm2:amd64 (2.4.101-2~18.04.1) ...\n",
      "Setting up libdrm-intel1:amd64 (2.4.101-2~18.04.1) ...\n",
      "Setting up libdrm-radeon1:amd64 (2.4.101-2~18.04.1) ...\n",
      "Setting up libdrm-nouveau2:amd64 (2.4.101-2~18.04.1) ...\n",
      "Setting up libdrm-amdgpu1:amd64 (2.4.101-2~18.04.1) ...\n",
      "Setting up libgl1-mesa-dri:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libglx-mesa0:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libglx0:amd64 (1.0.0-2ubuntu2.3) ...\n",
      "Setting up libgl1:amd64 (1.0.0-2ubuntu2.3) ...\n",
      "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9833b-0958-4fc5-a393-1f8a852bb98a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d4fee2-26a0-4706-bb47-cd5b344c0dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\" to /home/ControlNet-XS/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 470M/470M [00:07<00:00, 62.1MB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import scripts.control_utils as cu\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be14a47f-ac71-4e60-a52f-4befa3fb0b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_config = 'ControlNet-XS/configs/inference/sdxl/sdxl_encD_canny_48m.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70915bf0-38be-40e8-a791-23a6ded976ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 320, out-chn: 320, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 640, out-chn: 640, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 352, out-chn: 32, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 2 w/ 64 channels and 1 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is None and using 1 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is 2048 and using 1 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is None and using 1 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is 2048 and using 1 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 2 w/ 64 channels and 1 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is None and using 1 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is 2048 and using 1 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is None and using 1 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 64, context_dim is 2048 and using 1 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 704, out-chn: 64, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is None and using 2 heads with a dimension of 64.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 128, context_dim is 2048 and using 2 heads with a dimension of 64.\n",
      "BasicTransformerBlock is using checkpointing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7457d52bcb8649d2a4f4ca130ed35a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/939k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f4d653e3c34543a4bd69d897f1bd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/512k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7539e10da48b476287c4702d075ae369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e710f3d21940a29e4dc1e130739ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5abc9b3853445ba9f95309b5c2a665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510ac8248c2048039997ac52e4566e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenCLIPEmbedder with 123060480 params. Trainable: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a195a98944e4d1f8387cb3198808ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ip_pytorch_model.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = cu.create_model(path_to_config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e572a-4d58-4b08-87a5-df59bd9b60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'PATH/TO/IMAGES/Shoe.png'\n",
    "\n",
    "canny_high_th = 250\n",
    "canny_low_th = 100\n",
    "size = 768\n",
    "num_samples=2\n",
    "\n",
    "image = cu.get_image(image_path, size=size)\n",
    "edges = cu.get_canny_edges(image, low_th=canny_low_th, high_th=canny_high_th)\n",
    "\n",
    "samples, controls = cu.get_sdxl_sample(\n",
    "    guidance=edges,\n",
    "    ddim_steps=10,\n",
    "    num_samples=num_samples,\n",
    "    model=model,\n",
    "    shape=[4, size // 8, size // 8],\n",
    "    control_scale=0.95,\n",
    "    prompt='cinematic, shoe in the streets, made from meat, photorealistic shoe, highly detailed',\n",
    "    n_prompt='lowres, bad anatomy, worst quality, low quality',\n",
    ")\n",
    "\n",
    "\n",
    "Image.fromarray(cu.create_image_grid(samples)).save('SDXL_MyShoe.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
