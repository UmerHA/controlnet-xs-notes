{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda36831-a1e7-4c2f-b3ab-5f45dfaac14e",
   "metadata": {},
   "source": [
    "In this notebook, I run one step for TwoStreamControlModel to get intermediate outputs which I can then compare to my local run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b74b9e-7bbc-41ff-bc01-3d3055570a57",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43bd7c1-8c6b-4f3c-bf65-5398fdbbdc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # needed to make torch deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1c7d13-020e-4053-9754-7f2bf5b4829f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import scripts.control_utils as cu\n",
    "import torch\n",
    "from PIL import Image\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad3a198-093b-46fa-9cda-a13750fc9aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be14a47f-ac71-4e60-a52f-4befa3fb0b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_config = 'ControlNet-XS/configs/inference/sdxl/sdxl_encD_canny_48m.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6552dd-0ad5-4db2-b1a6-3c9b8ca55f4f",
   "metadata": {},
   "source": [
    "If this results in the kernel crashing, I'm using too much GPU memory elsewhere. Shut down every other kernel and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70915bf0-38be-40e8-a791-23a6ded976ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 320, out-chn: 320, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 640, out-chn: 640, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 10 w/ 1280 channels and 20 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 2 w/ 640 channels and 10 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 352, out-chn: 32, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 2 w/ 64 channels and 1 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 2 w/ 64 channels and 1 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [2048, 2048] now.\n",
      "Building a Downsample layer with 2 dims.\n",
      "  --> settings are: \n",
      " in-chn: 704, out-chn: 64, kernel-size: 3, stride: 2, padding: 1\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "constructing SpatialTransformer of depth 10 w/ 128 channels and 2 heads\n",
      "WARNING: SpatialTransformer: Found context dims [2048] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048] now.\n",
      "Initialized embedder #0: FrozenCLIPEmbedder with 123060480 params. Trainable: False\n",
      "Initialized embedder #1: FrozenOpenCLIPEmbedder2 with 694659841 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Loaded model config from [ControlNet-XS/configs/inference/sdxl/sdxl_encD_canny_48m.yaml]\n"
     ]
    }
   ],
   "source": [
    "model = cu.create_model(path_to_config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722ce2e4-2e2b-4f4c-bc1e-8818a7ae7c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 768\n",
    "num_samples=1\n",
    "prompt='cinematic, shoe in the streets, made from meat, photorealistic shoe, highly detailed'\n",
    "n_prompt='lowres, bad anatomy, worst quality, low quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05676d1d-23c9-4982-98e9-6142b876d136",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_canny_edges():\n",
    "    image_path = 'input_images/shoe.png' # chosen to fit size above\n",
    "    image = cu.get_image(image_path, size=size)\n",
    "    edges = cu.get_canny_edges(image, low_th=100, high_th=250)\n",
    "    return edges\n",
    "edges = get_canny_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e372b6-ecd3-4f26-a0dc-809795f24dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3bbdb7-8637-4c47-ad36-58f8e7ba57e5",
   "metadata": {},
   "source": [
    "Let's play with text encoders. Goal: Get all hidden states for both text encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5bae42-473f-4d91-b437-68754b1473b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import cls_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbadc75-092d-4260-afa5-7d939f4a2e91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FrozenCLIPEmbedder', 'FrozenOpenCLIPEmbedder2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_encoder_1 = model.conditioner.embedders[0]\n",
    "txt_encoder_2 = model.conditioner.embedders[1]\n",
    "cls_name(txt_encoder_1),cls_name(txt_encoder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2da953f5-5afd-4fd5-a7e7-66e4a8e1f35a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_of_FrozenCLIPEmbedder(self, text):\n",
    "    batch_encoding = self.tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=self.max_length,\n",
    "        return_length=True,\n",
    "        return_overflowing_tokens=False,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "    outputs = self.transformer(\n",
    "        input_ids=tokens, output_hidden_states=self.layer == \"hidden\"\n",
    "    )\n",
    "    if self.layer == \"last\":\n",
    "        z = outputs.last_hidden_state\n",
    "    elif self.layer == \"pooled\":\n",
    "        z = outputs.pooler_output[:, None, :]\n",
    "    else:\n",
    "        z = outputs.hidden_states[self.layer_idx]\n",
    "    if self.return_pooled:\n",
    "        return z, outputs.pooler_output\n",
    "    return z\n",
    "\n",
    "import open_clip\n",
    "def forward_of_FrozenOpenCLIPEmbedder2(self, text):\n",
    "    tokens = open_clip.tokenize(text)\n",
    "    z = self.encode_with_transformer(tokens.to(self.device))\n",
    "    if not self.return_pooled and self.legacy:\n",
    "        return z\n",
    "    if self.return_pooled:\n",
    "        assert not self.legacy\n",
    "        return z[self.layer], z[\"pooled\"]\n",
    "    return z[self.layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0757316-4872-4a95-91bb-7106f16f17ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.8918, -2.5114,  4.7184,  ...,  1.1266, -2.2387, -1.2507],\n",
       "         [ 0.2655, -0.0114,  0.6279,  ..., -0.5496, -0.1617, -0.1203],\n",
       "         [ 1.0162, -0.1704,  0.6562,  ..., -0.2048,  0.1218,  0.8765],\n",
       "         ...,\n",
       "         [ 0.2106, -0.4381, -0.0386,  ..., -1.1807, -0.8254, -0.1423],\n",
       "         [ 0.2137, -0.4490, -0.0561,  ..., -1.1906, -0.8379, -0.1446],\n",
       "         [ 0.1943, -0.4328,  0.0195,  ..., -1.2042, -0.8573, -0.1944]]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_1_txt = txt_encoder_1(prompt)\n",
    "encoded_1_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05933271-e0b3-4a0e-8617-c026f72cdad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.8917, -2.5113,  4.7167,  ...,  1.1270, -2.2396, -1.2505],\n",
       "         [ 0.2660, -0.0113,  0.6275,  ..., -0.5494, -0.1619, -0.1202],\n",
       "         [ 1.0159, -0.1706,  0.6558,  ..., -0.2045,  0.1210,  0.8754],\n",
       "         ...,\n",
       "         [ 0.2110, -0.4372, -0.0396,  ..., -1.1811, -0.8259, -0.1430],\n",
       "         [ 0.2137, -0.4484, -0.0568,  ..., -1.1910, -0.8385, -0.1451],\n",
       "         [ 0.1941, -0.4325,  0.0185,  ..., -1.2047, -0.8581, -0.1952]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_1_txt_manual = forward_of_FrozenCLIPEmbedder(txt_encoder_1, prompt)\n",
    "encoded_1_txt_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f100243-a14e-4d95-8f81-de293363b572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sgm.util import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d8b882-fd71-4f28-bfec-c201757eb7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.8918, -2.5114,  4.7184,  ...,  1.1266, -2.2387, -1.2507],\n",
       "         [ 0.2655, -0.0114,  0.6279,  ..., -0.5496, -0.1617, -0.1203],\n",
       "         [ 1.0162, -0.1704,  0.6562,  ..., -0.2048,  0.1218,  0.8765],\n",
       "         ...,\n",
       "         [ 0.2106, -0.4381, -0.0386,  ..., -1.1807, -0.8254, -0.1423],\n",
       "         [ 0.2137, -0.4490, -0.0561,  ..., -1.1906, -0.8379, -0.1446],\n",
       "         [ 0.1943, -0.4328,  0.0195,  ..., -1.2042, -0.8573, -0.1944]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_1_txt_manual_autocasted = autocast(forward_of_FrozenCLIPEmbedder)(txt_encoder_1, prompt)\n",
    "encoded_1_txt_manual_autocasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be4b2320-7fe5-45ec-99f1-09c8af5080e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoded_1_txt-encoded_1_txt_manual).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca59a4a-c440-4a35-8303-16c9610b67cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoded_1_txt-encoded_1_txt_manual_autocasted).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e1ae9e4-b55b-48c3-b391-c34d146ab126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_2_txt = txt_encoder_2(prompt)[0]  # 0 = text conditioning; 1 = text part of time conditioning\n",
    "encoded_2_txt_manual = forward_of_FrozenOpenCLIPEmbedder2(txt_encoder_2, prompt)[0]\n",
    "encoded_2_txt_manual_autocasted = autocast(forward_of_FrozenOpenCLIPEmbedder2)(txt_encoder_2, prompt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "602acaba-3cdf-4818-9467-5fa7045d30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0006, device='cuda:0'), tensor(0., device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoded_2_txt-encoded_2_txt_manual).abs().mean(), (encoded_2_txt-encoded_2_txt_manual_autocasted).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d2a4264-f0d2-477f-86ea-35731e0b57da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(encoded_1_txt_manual, 'intermediate_output/text emb/cloud_1.pt')\n",
    "torch.save(encoded_2_txt_manual, 'intermediate_output/text emb/cloud_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884639d-9e75-4569-80a6-4bdcce35d3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
