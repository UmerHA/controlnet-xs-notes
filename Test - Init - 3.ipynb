{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30794abe-5ec0-4ecd-90c5-5ef222fc2e48",
   "metadata": {},
   "source": [
    "This notebooks tests new code in diffusers: `ControlNetXSModel.__init__`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a44a64-2eaf-4b6b-8d0b-7514c7d5c2ff",
   "metadata": {},
   "source": [
    "### Create ControlNet-XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb778d99-86e1-417b-95b7-da9433821a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers.models.controlnetxs import ControlNetXSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8eff952-5d64-464c-827f-2c64902eca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnxs = ControlNetXSModel.create_as_in_paper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887f301c-9c95-4b87-a47a-b278a1fff72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(diffusers.models.unet_2d_condition.UNet2DConditionModel,\n",
       " diffusers.models.unet_2d_condition.UNet2DConditionModel)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cnxs.base_model),type(cnxs.control_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2b47b-a87c-401a-846f-f16e030ec4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4210c79d-dc7d-48ff-9f0f-d7418f71bc3f",
   "metadata": {},
   "source": [
    "### Prepare input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfa77c-ec7e-4d31-a8fa-61c74bba6cc4",
   "metadata": {},
   "source": [
    "I will need to use some preprocessing functions from `StableDiffusionControlNetPipeline`, so let create an instance\n",
    "\n",
    "(We need a `StableDiffusionControlNetPipeline` instead of a regular `StableDiffusionPipeline` because it has a `prepare_image` method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4787bc-8219-4b72-8067-084a7cf1a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import repeat\n",
    "from torch import tensor\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86d1e12-8148-4c5b-acad-09add1735652",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "device_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a51cfc-fafb-46c9-b3c0-b0ca5ede5f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6766107c9740c2886815514e05343b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "controlnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny', torch_dtype=device_dtype)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, torch_dtype=device_dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c298aaf9-54f8-4f06-8fd4-7d2b57e72c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_enc(txt):\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    return text_encoder(text_input.input_ids.to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "418a25a0-77a9-4f4c-a87c-f91ccf6e06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CannyDetector:\n",
    "    def __call__(self, img, low_threshold, high_threshold):\n",
    "        return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def get_canny_edges(image, size=512, threshold=(50, 200)):\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "    edges = CannyDetector()(image, *threshold)  # original sized greyscale edges\n",
    "    edges = edges / 255.\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1115e400-52b9-4ee7-81fb-09d1ef121f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(prompt, image, cnxs):\n",
    "    text_embeddings = text_enc(prompt)\n",
    "    \n",
    "    guidance_scale = 7.5\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "    \n",
    "    # 2. Define call parameters\n",
    "    batch_size = 1 # because prompt is a single string\n",
    "    num_images_per_prompt  = 1\n",
    "    \n",
    "    prompt_embeds, negative_prompt_embeds = pipe.encode_prompt(\n",
    "        prompt,\n",
    "        device,\n",
    "        num_images_per_prompt,\n",
    "        do_classifier_free_guidance,\n",
    "    )\n",
    "    prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "    prompt_embeds.shape\n",
    "    \n",
    "    # 6. Prepare latent variables\n",
    "    num_channels_latents = cnxs.base_model.config.in_channels # we're using our unet here!\n",
    "    num_channels_latents\n",
    "    \n",
    "    # Default values for prepare_image\n",
    "    height, width = None, None\n",
    "    generator = None\n",
    "    latents = None\n",
    "    guess_mode = False\n",
    "    \n",
    "    # 4. Prepare image\n",
    "    image = pipe.prepare_image(\n",
    "        image=image,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        batch_size=batch_size * num_images_per_prompt,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        device=device,\n",
    "        dtype=controlnet.dtype,\n",
    "        do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "        guess_mode=guess_mode,\n",
    "    )\n",
    "    height, width = image.shape[-2:]\n",
    "    height, width, image.shape\n",
    "    \n",
    "    # Default values for set_timesteps\n",
    "    num_inference_steps = 50\n",
    "    \n",
    "    # 5. Prepare timesteps\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = pipe.scheduler.timesteps\n",
    "    \n",
    "    # 6. Prepare latent variables\n",
    "    latents = pipe.prepare_latents(\n",
    "        batch_size * num_images_per_prompt,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        prompt_embeds.dtype,\n",
    "        device,\n",
    "        generator,\n",
    "        latents,\n",
    "    )\n",
    "    \n",
    "    latents.shape\n",
    "    \n",
    "    # 8. Denoising loop\n",
    "    num_warmup_steps = len(timesteps) - num_inference_steps * pipe.scheduler.order\n",
    "    \n",
    "    i,t = 0, timesteps[0] # NOTE: We only do 1 step for testing\n",
    "    \n",
    "    # expand the latents if we are doing classifier free guidance\n",
    "    latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "    \n",
    "    # controlnet(s) inference\n",
    "    # guess_mode == False\n",
    "    control_model_input = latent_model_input\n",
    "    controlnet_prompt_embeds = prompt_embeds\n",
    "    \n",
    "    hint_image = image\n",
    "    edges = get_canny_edges(hint_image)    \n",
    "    num_samples=2\n",
    "    edges = repeat(tensor(edges), 'h w -> b c h w', b=num_samples, c=3)\n",
    "\n",
    "    # x,t,c,context,hint\n",
    "    return latent_model_input, t, prompt_embeds, {}, edges.to(device, dtype=device_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f05951-afe6-46e3-a355-801652ae3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'A turtle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5dff421-b85a-46a7-8701-5ef3a8fc8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: original_image = load_image('https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png')\n",
    "except Exception: original_image = load_image('/Users/umer/Desktop/input_image_vermeer.png')\n",
    "image = original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff754e32-66d3-4275-b3d3-8ceb7e79c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,t,c,context,hint = prepare_input(prompt,image,cnxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a947c31-941d-4c37-bba2-96c2f06fa79b",
   "metadata": {},
   "source": [
    "### Run ControlNet-XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d672bb99-c0e9-411b-952e-50a296ce806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cnxs(x,t,c,context,hint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02b9d0c4-b582-4f59-a536-2be2f5f4f3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 64, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140db8c7-887c-4261-a9cf-a23caeade166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
