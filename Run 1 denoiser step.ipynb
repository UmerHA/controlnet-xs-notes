{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caaa193-85fa-4fdd-9c2c-efffbb0d24d8",
   "metadata": {},
   "source": [
    "Use this notebook to run 1 denoiser step and get updated intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ac7b3d-0670-48ba-83b5-b0fc2c96c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.testing import assert_close\n",
    "from torch import allclose, nn, tensor\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc434cf-df05-443c-875b-cf14fecf56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "device_dtype = torch.float16 if device == 'cuda' else torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae8cee-8c49-4616-a057-cb04c2379f9d",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eadcacd0-caa8-40b4-8382-484a77d64e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "from diffusers.models.controlnetxs import ControlNetXSModel\n",
    "from diffusers.pipelines.controlnet_xs.pipeline_controlnet_xs_sd_xl import StableDiffusionXLControlNetXSPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e16edb-e1a7-4c84-9e85-eafb99f3e6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdxl_pipe = StableDiffusionXLPipeline.from_single_file('weights/sdxl/sd_xl_base_1.0_0.9vae.safetensors').to(device)\n",
    "cnxs = ControlNetXSModel.from_pretrained('weights/cnxs').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55c488d-6a16-4fff-8ab8-c0e3e07d30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnxs.base_model = sdxl_pipe.unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a2cab-f0b6-4865-b1a2-9b187c288671",
   "metadata": {},
   "source": [
    "The example script of Heidelberg manually sets scale_list to 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb82c0b4-0266-421b-b9fd-f03a8e5d33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnxs.scale_list = cnxs.scale_list * 0. + 0.95\n",
    "assert cnxs.scale_list[0] == .95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c4cfb-2ad6-4aea-a424-bdf3ed1810a2",
   "metadata": {},
   "source": [
    "Heidelberg uses `timestep_spacing = 'linspace'` in their scheduler, so let's do that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba85e97-5306-49a4-87e3-4f6e8add9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep_spacing = \"leading\" and timesteps=[999.      978.61224 958.2245  937.83673 917.449  ] ...\n",
      "sigmas before interpolation: [0.02916753 0.04131448 0.05068044 0.05861427 0.06563709] ...\n",
      "sigmas after (linear) interpolation: [14.61464691 12.93677721 11.49164976 10.24291444  9.16035419] ...\n",
      "At end of `set_timesteps`:\n",
      "sigmas =  tensor([14.6146, 12.9368, 11.4916, 10.2429,  9.1604]) ...\n",
      "timesteps = tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490]) ...\n"
     ]
    }
   ],
   "source": [
    "scheduler_cgf = dict(sdxl_pipe.scheduler.config)\n",
    "scheduler_cgf['timestep_spacing'] = 'linspace'\n",
    "sdxl_pipe.scheduler = EulerDiscreteScheduler.from_config(scheduler_cgf)\n",
    "\n",
    "# test it worked\n",
    "sdxl_pipe.scheduler.set_timesteps(50)\n",
    "assert sdxl_pipe.scheduler.timesteps[0]==999\n",
    "\n",
    "# reset\n",
    "sdxl_pipe.scheduler = EulerDiscreteScheduler.from_config(scheduler_cgf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc68274d-afea-47b5-98ad-24b58419e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnxs_pipe = StableDiffusionXLControlNetXSPipeline(\n",
    "    vae=sdxl_pipe.vae,\n",
    "    text_encoder=sdxl_pipe.text_encoder,\n",
    "    text_encoder_2=sdxl_pipe.text_encoder_2,\n",
    "    tokenizer=sdxl_pipe.tokenizer,\n",
    "    tokenizer_2=sdxl_pipe.tokenizer_2,\n",
    "    unet=sdxl_pipe.unet,\n",
    "    controlnet=cnxs,\n",
    "    scheduler=sdxl_pipe.scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292f02d-22a4-433a-ac78-48d954d296c3",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35062f5c-f174-4ab3-a107-298ad02b66c8",
   "metadata": {},
   "source": [
    "## Run 1 step locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b7698e-b254-4e98-bb23-a7c3def06d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers.utils import load_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CannyDetector:\n",
    "    def __call__(self, img, low_threshold, high_threshold):\n",
    "        return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def get_canny_edges(image, threshold=(100, 250)):\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "    edges = CannyDetector()(image, *threshold)  # original sized greyscale edges\n",
    "    edges = edges / 255.\n",
    "    return edges\n",
    "\n",
    "def seed_everything(seed):\n",
    "    # paper used deprecated `seed_everything` from pytorch lightning\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "RANDOM_SEED_IN_PAPER = 1999158951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715b742a-679a-4e33-a564-f98f1eb22420",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_sdxl_cloud = torch.load('latents_cloud_no_control.pth', map_location=torch.device(device))\n",
    "rand_from_cloud = latents_sdxl_cloud[0] / 14.6146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c91cb8d-0097-43f2-bec0-217d21f4a814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'cinematic, shoe in the streets, made from meat, photorealistic shoe, highly detailed'\n",
    "neg_prompt = 'lowres, bad anatomy, worst quality, low quality'\n",
    "\n",
    "image = load_image('input_images/shoe_cloud.png')\n",
    "edges = get_canny_edges(image)\n",
    "\n",
    "edges_tensor = torch.tensor(edges)\n",
    "three_edges = torch.stack((edges_tensor,edges_tensor,edges_tensor))\n",
    "three_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35dc4f64-aba9-4a1e-8b82-35b7d0d82cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep_spacing = \"leading\" and timesteps=[999.      978.61224 958.2245  937.83673 917.449  ] ...\n",
      "sigmas before interpolation: [0.02916753 0.04131448 0.05068044 0.05861427 0.06563709] ...\n",
      "sigmas after (linear) interpolation: [14.61464691 12.93677721 11.49164976 10.24291444  9.16035419] ...\n",
      "At end of `set_timesteps`:\n",
      "sigmas =  tensor([14.6146, 12.9368, 11.4916, 10.2429,  9.1604], device='mps:0') ...\n",
      "timesteps = tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490], device='mps:0') ...\n",
      "Passed in latents:  tensor([ 1.3333,  0.5155,  0.4647, -0.5344,  1.0102], device='mps:0')\n",
      "initial_unscaled_latents:  tensor([ 1.3333,  0.5155,  0.4647, -0.5344,  1.0102], device='mps:0')\n",
      "latents:  tensor([19.4858,  7.5339,  6.7910, -7.8098, 14.7639], device='mps:0')\n",
      "add_time_ids = tensor([[768., 768.,   0.,   0., 768., 768.],\n",
      "        [768., 768.,   0.,   0., 768., 768.]], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1206422f4fa64a0782c0dbfec9973e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1] tensor([ 0.1755,  0.1357, -0.3653,  0.4109, -0.2517,  0.6668, -0.6832,  0.1072, -0.3240,  0.2147], device='mps:0')\n",
      "A2] tensor([-0.0147, -0.3285, -0.0927,  0.0223,  0.0955, -0.2987,  0.0222,  0.0760, -0.0548, -0.7091], device='mps:0')\n",
      "A3] tensor([ 0.1755,  0.1357, -0.3653,  0.4109, -0.2517,  0.6668, -0.6832,  0.1072, -0.3240,  0.2147], device='mps:0')\n",
      "B1] tensor([ 0.3032,  0.2355,  0.2123,  0.2237,  0.4928,  0.0095,  0.3033,  0.3297,  0.0226, -0.1435], device='mps:0')\n",
      "C1] tensor([-0.5794, -0.3963, -0.1934, -0.6616, -0.0560, -0.1097, -0.4058,  0.0836, -0.6489, -1.1214], device='mps:0')\n",
      "D1] tensor([0.0524, 0.0221, 0.0505, 0.0232, 0.0388, 0.0288, 0.0666, 0.0229, 0.0460, 0.0424], device='mps:0')\n",
      "D2] tensor([0.9500], device='mps:0')\n",
      "D3] tensor([ 0.3530,  0.2566,  0.2602,  0.2458,  0.5296,  0.0368,  0.3665,  0.3515,  0.0663, -0.1032], device='mps:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Alright captain, do your analysis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m seed_everything(RANDOM_SEED_IN_PAPER)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcnxs_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneg_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthree_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrand_from_cloud\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/controlnet-xs-ngkyeIE7/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/diffusers/diffusers/src/diffusers/pipelines/controlnet_xs/pipeline_controlnet_xs_sd_xl.py:753\u001b[0m, in \u001b[0;36mStableDiffusionXLControlNetXSPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, image, height, width, num_inference_steps, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size)\u001b[0m\n\u001b[1;32m    750\u001b[0m added_cond_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m: add_text_embeds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: add_time_ids}\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrolnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# todo: better naming\u001b[39;49;00m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#return_dict=False,\u001b[39;49;00m\n\u001b[1;32m    761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    763\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/controlnet-xs-ngkyeIE7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/controlnet-xs-ngkyeIE7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/diffusers/diffusers/src/diffusers/models/controlnetxs.py:437\u001b[0m, in \u001b[0;36mControlNetXSModel.forward\u001b[0;34m(self, x, t, encoder_hidden_states, hint, cross_attention_kwargs, added_cond_kwargs, no_control)\u001b[0m\n\u001b[1;32m    435\u001b[0m     hs_base\u001b[38;5;241m.\u001b[39mappend(h_base)\n\u001b[1;32m    436\u001b[0m     hs_ctrl\u001b[38;5;241m.\u001b[39mappend(h_ctrl)\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlright captain, do your analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m h_ctrl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([h_ctrl, h_base], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    439\u001b[0m debug_by_umer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_ctrl\u001b[39m\u001b[38;5;124m'\u001b[39m, h_ctrl)\n",
      "\u001b[0;31mValueError\u001b[0m: Alright captain, do your analysis"
     ]
    }
   ],
   "source": [
    "seed_everything(RANDOM_SEED_IN_PAPER)\n",
    "cnxs_pipe(prompt, negative_prompt=neg_prompt,image=three_edges, latents=rand_from_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3adc4a1-2f0f-4c4f-9195-a7a643e779bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
